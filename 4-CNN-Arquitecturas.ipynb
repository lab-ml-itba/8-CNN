{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitecturas de las CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[My recomendado!](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ImageNet](http://image-net.org/index)\n",
    "- Base de datos de imágenes\n",
    "- Organizada de acuerdo a la jerarquía WordNet (synonyms called synsets). Hay mas de 100.000 synsets de los cuales mas de 80.000 son sinónimos\n",
    "- En promedio 1000 imágenes para describir cada synset\n",
    "- [Estadisticas del dataset](http://image-net.org/about-stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Competencias desde 2010](http://www.image-net.org/challenges/LSVRC/) ImageNet Large Scale Visual Recognition Challenge (LSVRC)\n",
    "Dieron como resultados de las investigaciones una serie de arquitecturas muy populares\n",
    "### AlexNet [(paper)](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "- 2012 \n",
    "- Error de top-5 de 15.3%, más de 10.8% arriba del segundo. \n",
    "- ImageNet: En 1.2 millones de imágenes de alta resolución, 1000 clases\n",
    "- Diseñada por: SuperVision group, consisting of Alex Krizhevsky, Geoffrey Hinton, and Ilya Sutskever.\n",
    "\n",
    "**Lo novedoso**: Dropout, RELUs\n",
    "\n",
    "**Arquitectura:**\n",
    "- CNN\n",
    "- Down-sampled a 256 × 256 (Originales en alta resolución)\n",
    "- Luego del data augmentation quedan en 224 x 224\n",
    "- Rectangulares con center-crop (Lado mas corto 256)\n",
    "- 60 millones de parámetros\n",
    "- 650 mil neuronas\n",
    "- 5 capas convolucionales\n",
    "- Algunos seguidos de capas pooling\n",
    "- 3 capas densas\n",
    "- SoftMax de 1000 categorias (1000 neuronas a la salida)\n",
    "- RELUs en vez de Sigmoideas\n",
    "- Dropout para evitar regularización\n",
    "\n",
    "**Entrenamiento:**\n",
    "- CUDA para soporte de GPU\n",
    "- Data augmentation\n",
    "- SGD\n",
    "- Batch size = 128\n",
    "- Momentum 0.9\n",
    "- Learning rate = 0.01\n",
    "- weight decay of 0.0005 (equivalente a L2 regularization)\n",
    "- 5 a 6 días de entrenamiento en 2 [GTX 580](https://www.geforce.com/hardware/desktop-gpus/geforce-gtx-580) 3GB GPUs\n",
    "\n",
    "**Detalles capas convolucionales**\n",
    "- Capa Convolucional 1: entrada: 224×224×3, 96 Kernels de 11x11x3 con stride de 4\n",
    "- Pooling\n",
    "- Capa Convolucional 2: 256 Kernels de 5x5x48\n",
    "- Capa Convolucional 3: 384 Kernels de 3x3x256\n",
    "- Capa Convolucional 4: 384 Kernels de 3x3x192\n",
    "- Capa Convolucional 5: 256 Kernels de 3x3x192\n",
    "- Denses layers: 4096 neuronas ambos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](alexNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementaciones en Keras:**\n",
    "- [CaffeNet - One stream](https://gist.github.com/JBed/c2fb3ce8ed299f197eff) - Caffe (Framework de Berkeley)\n",
    "- [Original - Two streams](https://github.com/dandxy89/ImageModels/blob/master/AlexNet_Original.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alexnet_weights.jpeg](alexnet_weights.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [VGG](https://arxiv.org/pdf/1409.1556.pdf) - Visial Geometry Group - Oxford\n",
    "- 2014\n",
    "- Dos tipos 16 y 19 (16/19 capas)\n",
    "- 7.3% error rate\n",
    "\n",
    "**Lo novedoso**: Kernels de 3x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGGNet.png](VGGNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogLeNet (2015) - Inception, Xception\n",
    "https://www.youtube.com/watch?v=HunX473yXEI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- top 5 error rate of 6.7%\n",
    "- Usa 9 Inception modules (Parallel Streams) en la arquitectura total con mas de 100 layers en total\n",
    "- No usa fully connected layers! Usa GAP para ir de 7x7x1024 a 1x1x1024 volume. \n",
    "- Usa 12x menos parametros que AlexNet.\n",
    "- Multiples versiones\n",
    "- Entrenada con GPUs durante una semana\n",
    "- https://www.tensorflow.org/tutorials/image_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GoogLeNet2.png](GoogLeNet2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Inception.png](Inception.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detalles de Inception y Xception:\n",
    "- https://medium.com/towards-data-science/an-intuitive-guide-to-deep-network-architectures-65fdc477db41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet (2015)\n",
    "https://www.youtube.com/watch?v=K0uoBKBQ1gA\n",
    "- “Ultra-deep” – Yann LeCun.\n",
    "- 152 layers\n",
    "- Trained on an 8 GPU machine for two to three weeks.\n",
    "- 3.6% error rate\n",
    "- Microsoft\n",
    "- Vanishing Gradient (Resuelto puenteando)\n",
    "- Concepto de bloque residual (skip connections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![resnets.png](resnets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Disponibles en Keras](https://keras.io/applications/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
